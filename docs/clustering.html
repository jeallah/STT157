<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5 Clustering | AniCom: Anime Recommendation System using Collaborative Filtering</title>
  <meta name="description" content="5 Clustering | AniCom: Anime Recommendation System using Collaborative Filtering" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="5 Clustering | AniCom: Anime Recommendation System using Collaborative Filtering" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5 Clustering | AniCom: Anime Recommendation System using Collaborative Filtering" />
  
  
  

<meta name="author" content="Jeallah Christine C. Cabrillas" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="collaborative-filtering.html"/>
<link rel="next" href="summary-and-conclusion.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>




</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#background-of-the-study"><i class="fa fa-check"></i><b>1.1</b> Background of the Study</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#statement-of-the-problem"><i class="fa fa-check"></i><b>1.2</b> Statement of the Problem</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#significance-of-the-study"><i class="fa fa-check"></i><b>1.3</b> Significance of the study</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#scope-and-delimitation"><i class="fa fa-check"></i><b>1.4</b> Scope and delimitation</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="review-of-related-literature.html"><a href="review-of-related-literature.html"><i class="fa fa-check"></i><b>2</b> Review of Related Literature</a></li>
<li class="chapter" data-level="3" data-path="recommendation-system.html"><a href="recommendation-system.html"><i class="fa fa-check"></i><b>3</b> Recommendation System</a></li>
<li class="chapter" data-level="4" data-path="collaborative-filtering.html"><a href="collaborative-filtering.html"><i class="fa fa-check"></i><b>4</b> Collaborative Filtering</a>
<ul>
<li class="chapter" data-level="4.1" data-path="collaborative-filtering.html"><a href="collaborative-filtering.html#user-based-collaborative-filtering-ubcf"><i class="fa fa-check"></i><b>4.1</b> User-based Collaborative Filtering (UBCF)</a></li>
<li class="chapter" data-level="4.2" data-path="collaborative-filtering.html"><a href="collaborative-filtering.html#item-based-collaborative-filtering-ibcf"><i class="fa fa-check"></i><b>4.2</b> Item-based Collaborative Filtering (IBCF)</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>5</b> Clustering</a>
<ul>
<li class="chapter" data-level="5.1" data-path="clustering.html"><a href="clustering.html#partition-based-algorithms"><i class="fa fa-check"></i><b>5.1</b> Partition-based Algorithms</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="clustering.html"><a href="clustering.html#k-means-algorithm"><i class="fa fa-check"></i><b>5.1.1</b> K-means Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="clustering.html"><a href="clustering.html#hierarchical-based-algorithms"><i class="fa fa-check"></i><b>5.2</b> Hierarchical-based Algorithms</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="clustering.html"><a href="clustering.html#agglomerative-clustering"><i class="fa fa-check"></i><b>5.2.1</b> Agglomerative Clustering</a></li>
<li class="chapter" data-level="5.2.2" data-path="clustering.html"><a href="clustering.html#divisive-clustering"><i class="fa fa-check"></i><b>5.2.2</b> Divisive Clustering</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="clustering.html"><a href="clustering.html#density-based-algorithms"><i class="fa fa-check"></i><b>5.3</b> Density-based Algorithms</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="clustering.html"><a href="clustering.html#dbscan-density-based-spatial-clustering-of-applications-with-noise"><i class="fa fa-check"></i><b>5.3.1</b> DBSCAN (Density-Based Spatial Clustering of Applications with Noise)</a></li>
<li class="chapter" data-level="5.3.2" data-path="clustering.html"><a href="clustering.html#optics-ordering-points-to-identify-clustering-structure"><i class="fa fa-check"></i><b>5.3.2</b> OPTICS (Ordering Points to Identify Clustering Structure)</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="clustering.html"><a href="clustering.html#grid-based-algorithms"><i class="fa fa-check"></i><b>5.4</b> Grid-based Algorithms</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="clustering.html"><a href="clustering.html#sting-statistical-information-grid-approach"><i class="fa fa-check"></i><b>5.4.1</b> STING (Statistical Information Grid Approach)</a></li>
<li class="chapter" data-level="5.4.2" data-path="clustering.html"><a href="clustering.html#wave-cluster"><i class="fa fa-check"></i><b>5.4.2</b> Wave Cluster</a></li>
<li class="chapter" data-level="5.4.3" data-path="clustering.html"><a href="clustering.html#clique-clustering-in-quest"><i class="fa fa-check"></i><b>5.4.3</b> CLIQUE (Clustering in Quest)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="summary-and-conclusion.html"><a href="summary-and-conclusion.html"><i class="fa fa-check"></i>Summary and Conclusion</a></li>
<li class="chapter" data-level="" data-path="plan-for-this-thesis.html"><a href="plan-for-this-thesis.html"><i class="fa fa-check"></i>Plan for this Thesis</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References:</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">AniCom: Anime Recommendation System using Collaborative Filtering</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="clustering" class="section level1" number="5">
<h1><span class="header-section-number">5</span> Clustering</h1>
<div class="line-block">       Clustering algorithms aim to group the fingerprints in classes of similar elements. The clustering requires the concept of a metric. These algorithms implement the straightforward assumption that similar data belongs to the same class. In cluster analysis, no assumption is made about the number of classes and their structure (statistical distribution); rather, the number of classes can be defined from the result of the analysis. Several different clustering techniques have been developed (di Natale &amp; Martinelli, 2019): partition-based algorithms hierarchical-based algorithms, density-based algorithms, and grid-based algorithms.</div>
<div id="partition-based-algorithms" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> Partition-based Algorithms</h2>
<div class="line-block">       Partitional clustering assigns a set of data points into k-clusters by using iterative processes. In these processes, n data are classified into k-clusters. The predefined criterion function J assigns the datum into kth number set according to the maximization and minimization calculation in k sets.</div>
<div id="k-means-algorithm" class="section level3" number="5.1.1">
<h3><span class="header-section-number">5.1.1</span> K-means Algorithm</h3>
<div class="line-block">      The K-Means clusters were first developed by Mac Queen (1967). In the K-Means clusters, clusters are formed using Euclidean distance. In the K-Means algorithm, unsupervised learning is used and k classes are created which minimizes the error function. In K-Means clustering, k cluster centers are created from the selected data set. It is then placed at the nearest cluster using Euclidean distance. New cluster centers are formed according to the results of the clustering. From the calculations of the clustering, the cluster center is recalculated. The arithmetic average is used as the calculation method, and the new cluster center is determined. All samples are reclassified according to the new center. This process is repeated until it is determined that the samples in the set have not passed to another set.</div>
</div>
</div>
<div id="hierarchical-based-algorithms" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> Hierarchical-based Algorithms</h2>
<div class="line-block">     In data mining and statistics, hierarchical clustering analysis is a method of cluster analysis that seeks to build a hierarchy of clusters i.e. tree-type structure based on the hierarchy. There are two types of hierarchical cluster analysis strategies, agglomerative clustering, and divisive clustering (GeeksforGeeks, 2021).</div>
<div id="agglomerative-clustering" class="section level3" number="5.2.1">
<h3><span class="header-section-number">5.2.1</span> Agglomerative Clustering</h3>
<div class="line-block">      Agglomerative Clustering: Also known as bottom-up approach or hierarchical agglomerative clustering (HAC). A structure that is more informative than the unstructured set of clusters returned by flat clustering. This clustering algorithm does not require us to pre-specify the number of clusters. Bottom-up algorithms treat each data as a singleton cluster at the outset and then successively agglomerates pairs of clusters until all clusters have been merged into a single cluster that contains all data (GeeksforGeeks, 2021).</div>
</div>
<div id="divisive-clustering" class="section level3" number="5.2.2">
<h3><span class="header-section-number">5.2.2</span> Divisive Clustering</h3>
<div class="line-block">     Divisive clustering: Also known as a top-down approach. This algorithm also does not require pre-specify the number of clusters. Top-down clustering requires a method for splitting a cluster that contains the whole data and proceeds by splitting clusters recursively until individual data have been split into singleton clusters (GeeksforGeeks, 2021).</div>
</div>
</div>
<div id="density-based-algorithms" class="section level2" number="5.3">
<h2><span class="header-section-number">5.3</span> Density-based Algorithms</h2>
<div class="line-block">       Density-Based Clustering refers to unsupervised learning methods that identify distinctive groups/clusters in the data, based on the idea that a cluster in data space is a contiguous region of high point density, separated from other such clusters by contiguous regions of low point density (University of New South Wales, 2011). The data points in the separating regions of low point density are typically considered noise/outliers. The clusters created in these methods can be of arbitrary shape. Following are the examples of Density-based clustering algorithms (Sharma, 2021).</div>
<div id="dbscan-density-based-spatial-clustering-of-applications-with-noise" class="section level3" number="5.3.1">
<h3><span class="header-section-number">5.3.1</span> DBSCAN (Density-Based Spatial Clustering of Applications with Noise)</h3>
<div class="line-block">      DBSCAN groups data points together based on the distance metric and criterion for a minimum number of data points. It takes two parameters – eps and minimum points. Eps indicates how close the data points should be to be considered as neighbors. The criterion for minimum points should be completed to consider that region as a dense region (Sharma, 2021).</div>
</div>
<div id="optics-ordering-points-to-identify-clustering-structure" class="section level3" number="5.3.2">
<h3><span class="header-section-number">5.3.2</span> OPTICS (Ordering Points to Identify Clustering Structure)</h3>
<div class="line-block">       It is similar in process to DBSCAN, but it attends to one of the drawbacks of the former algorithm i.e. inability to form clusters from data of arbitrary density. It considers two more parameters which are core distance and reachability distance. Core distance indicates whether the data point being considered is core or not by setting a minimum value for it.</div>
<p>Reachability distance is the maximum of core distance and the value of distance metric that is used for calculating the distance among two data points. One thing to consider about reachability distance is that its value remains not defined if one of the data points is a core point (Sharma, 2021).
2.3.3.3 HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise)
HDBSCAN is a density-based clustering method that extends the DBSCAN methodology by converting it to a hierarchical clustering algorithm (Sharma, 2021).</p>
</div>
</div>
<div id="grid-based-algorithms" class="section level2" number="5.4">
<h2><span class="header-section-number">5.4</span> Grid-based Algorithms</h2>
<div class="line-block">    In grid-based clustering, the data set is represented into a grid structure that comprises of grids (also called cells). The overall approach in the algorithms of this method differs from the rest of the algorithms.</div>
<p>They are more concerned with the value space surrounding the data points rather than the data points themselves. One of the greatest advantages of these algorithms is their reduction in computational complexity. This makes it appropriate for dealing with humongous data sets.
After partitioning the data sets into cells, it computes the density of the cells which helps in identifying the clusters. A few algorithms based on grid-based clustering are as follows (Sharma, 2021). : –</p>
<div id="sting-statistical-information-grid-approach" class="section level3" number="5.4.1">
<h3><span class="header-section-number">5.4.1</span> STING (Statistical Information Grid Approach)</h3>
<div class="line-block">    In STING, the data set is divided recursively in a hierarchical manner. Each cell is further sub-divided into a different number of cells. It captures the statistical measures of the cells which helps in answering the queries in a small amount of time (Sharma, 2021).</div>
</div>
<div id="wave-cluster" class="section level3" number="5.4.2">
<h3><span class="header-section-number">5.4.2</span> Wave Cluster</h3>
<div class="line-block">       In this algorithm, the data space is represented in form of wavelets. The data space composes an n-dimensional signal which helps in identifying the clusters. The parts of the signal with a lower frequency and high amplitude indicate that the data points are concentrated. These regions are identified as clusters by the algorithm. The parts of the signal where the frequency high represents the boundaries of the clusters (Sharma, 2021).</div>
</div>
<div id="clique-clustering-in-quest" class="section level3" number="5.4.3">
<h3><span class="header-section-number">5.4.3</span> CLIQUE (Clustering in Quest)</h3>
<div class="line-block">       CLIQUE is a combination of density-based and grid-based clustering algorithm. It partitions the data space and identifies the sub-spaces using the Apriori principle. It identifies the clusters by calculating the densities of the cells (Sharma, 2021)</div>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="collaborative-filtering.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="summary-and-conclusion.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
