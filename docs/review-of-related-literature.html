<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 Review of Related Literature | AniCom: Anime Recommendation System using Collaborative Filtering</title>
  <meta name="description" content="2 Review of Related Literature | AniCom: Anime Recommendation System using Collaborative Filtering" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="2 Review of Related Literature | AniCom: Anime Recommendation System using Collaborative Filtering" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 Review of Related Literature | AniCom: Anime Recommendation System using Collaborative Filtering" />
  
  
  

<meta name="author" content="Jeallah Christine C. Cabrillas" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction.html"/>
<link rel="next" href="summary-and-conclusion.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>




</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#background-of-the-study"><i class="fa fa-check"></i><b>1.1</b> 1.1 Background of the Study</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#statement-of-the-problem"><i class="fa fa-check"></i><b>1.2</b> 1.2 Statement of the Problem</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#significance-of-the-study"><i class="fa fa-check"></i><b>1.3</b> 1.3 Significance of the study</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#scope-and-delimitation"><i class="fa fa-check"></i><b>1.4</b> 1.4 Scope and delimitation</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="review-of-related-literature.html"><a href="review-of-related-literature.html"><i class="fa fa-check"></i><b>2</b> Review of Related Literature</a>
<ul>
<li class="chapter" data-level="2.1" data-path="review-of-related-literature.html"><a href="review-of-related-literature.html#recommendation-system"><i class="fa fa-check"></i><b>2.1</b> 2.1 Recommendation System</a></li>
<li class="chapter" data-level="2.2" data-path="review-of-related-literature.html"><a href="review-of-related-literature.html#collaborative-filtering"><i class="fa fa-check"></i><b>2.2</b> 2.2 Collaborative Filtering</a></li>
<li class="chapter" data-level="2.3" data-path="review-of-related-literature.html"><a href="review-of-related-literature.html#user-based-collaborative-filtering-ubcf"><i class="fa fa-check"></i><b>2.3</b> 2.2.1 User-based Collaborative Filtering (UBCF)</a></li>
<li class="chapter" data-level="2.4" data-path="review-of-related-literature.html"><a href="review-of-related-literature.html#item-based-collaborative-filtering-ibcf"><i class="fa fa-check"></i><b>2.4</b> 2.2.2 Item-based Collaborative Filtering (IBCF)</a></li>
<li class="chapter" data-level="2.5" data-path="review-of-related-literature.html"><a href="review-of-related-literature.html#clustering"><i class="fa fa-check"></i><b>2.5</b> 2.3 Clustering</a></li>
<li class="chapter" data-level="2.6" data-path="review-of-related-literature.html"><a href="review-of-related-literature.html#partition-based-algorithms"><i class="fa fa-check"></i><b>2.6</b> 2.3.1 Partition-based Algorithms</a></li>
<li class="chapter" data-level="2.7" data-path="review-of-related-literature.html"><a href="review-of-related-literature.html#k-means-algorithm"><i class="fa fa-check"></i><b>2.7</b> 2.3.1.1 K-means Algorithm</a></li>
<li class="chapter" data-level="2.8" data-path="review-of-related-literature.html"><a href="review-of-related-literature.html#hierarchical-based-algorithms"><i class="fa fa-check"></i><b>2.8</b> 2.3.2 Hierarchical-based Algorithms</a></li>
<li class="chapter" data-level="2.9" data-path="review-of-related-literature.html"><a href="review-of-related-literature.html#agglomerative-clustering"><i class="fa fa-check"></i><b>2.9</b> 2.3.2.1 Agglomerative Clustering</a></li>
<li class="chapter" data-level="2.10" data-path="review-of-related-literature.html"><a href="review-of-related-literature.html#divisive-clustering"><i class="fa fa-check"></i><b>2.10</b> 2.3.2.2 Divisive Clustering</a></li>
<li class="chapter" data-level="2.11" data-path="review-of-related-literature.html"><a href="review-of-related-literature.html#density-based-algorithms"><i class="fa fa-check"></i><b>2.11</b> 2.3.3 Density-based Algorithms</a></li>
<li class="chapter" data-level="2.12" data-path="review-of-related-literature.html"><a href="review-of-related-literature.html#dbscan-density-based-spatial-clustering-of-applications-with-noise"><i class="fa fa-check"></i><b>2.12</b> 2.3.3.1 DBSCAN (Density-Based Spatial Clustering of Applications with Noise)</a></li>
<li class="chapter" data-level="2.13" data-path="review-of-related-literature.html"><a href="review-of-related-literature.html#optics-ordering-points-to-identify-clustering-structure"><i class="fa fa-check"></i><b>2.13</b> 2.3.3.2 OPTICS (Ordering Points to Identify Clustering Structure)</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="summary-and-conclusion.html"><a href="summary-and-conclusion.html"><i class="fa fa-check"></i><b>3</b> Summary and Conclusion</a></li>
<li class="chapter" data-level="4" data-path="plan-for-this-thesis.html"><a href="plan-for-this-thesis.html"><i class="fa fa-check"></i><b>4</b> Plan for this Thesis</a></li>
<li class="chapter" data-level="5" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>5</b> References:</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">AniCom: Anime Recommendation System using Collaborative Filtering</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="review-of-related-literature" class="section level1" number="2">
<h1><span class="header-section-number">2</span> Review of Related Literature</h1>
<div id="recommendation-system" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> 2.1 Recommendation System</h2>
<pre><code>Recommender systems are the systems that are designed to recommend things to the user based on many different factors. These systems predict the most likely product that the users are most likely to purchase and are of interest to (Dwivedi, 2020). A recommender system is a type of information filtering system (Severt, 2020). It deals with a large volume of information present by filtering the most important information based on the data provided by a user and other factors that take care of the user’s preference and interest. It finds out the match between user and item and imputes the similarities between users and items for recommendation (Dwivedi, 2020). By drawing from huge data sets, the system’s algorithm can pinpoint accurate user preferences. Once you know what your users like, you can recommend new, relevant content.  And that’s true for everything from movies and music, to romantic partners. Netflix, YouTube, Tinder, and Amazon are all examples of recommender systems in use. The systems entice users with relevant suggestions based on the choices they make (Severt, 2020). Recommender systems combine ideas from user profiling, information filtering, and machine learning to deliver users a more intelligent and proactive information service by making concrete product or service recommendations that match their learned user preferences and needs. The recommender technology is superior to other information filtering applications because of its ability to provide personalized and meaningful information recommendations. For example, while standard search engines are very likely to generate the same results to different users entering identical search queries, recommender systems are able to generate results to each user that are personalized and more relevant because they take into account each user’s interests (Zhou et al., 2011). A Recommender System is composed of two modules: a database and a filtering technique. The database is responsible for storing information about users, items, and the associated ratings. The filtering technique is composed of an algorithm, generally split into two stages. On the first, the similarities between users or items are identified providing a neighborhood of each item or user. At the second stage, the system predicts ratings corresponding to the items unseen by the user, and only the best ones are used as recommendations (Bobadilla et al., 2013). As a result of research, several types of filtering algorithms have been developed: Collaborative filtering, Content-based filtering, and Hybrid filtering (Zhou et al., 2011). Content-based recommender systems store content information about each item to be recommended. This information will be used to recommend items similar to the ones the user preferred in the past, based on how similar certain items are to each other or the similarity with respect to user preferences (also represented by means of a subset of content features). Collaborative filtering recommender systems attempt to identify groups of people with similar tastes to those of the user and recommend items that they have liked. And, Hybrid recommender systems combine both content and collaborative approaches. Different combination methods could be used from running both alternatives separately and combining their prediction to construct a general unifying model which incorporates both models (de Campos et al., 2008).</code></pre>
</div>
<div id="collaborative-filtering" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> 2.2 Collaborative Filtering</h2>
<pre><code>Collaborative filtering is a type of personalized recommendation strategy that identifies the similarities between users (based on site interactions) to serve relevant product recommendations across digital properties. Recommender systems collect user information, mining this data to inform which items to display (Retta, 2020). Collaborative Filtering doesn’t need a good amount of information of item’s features (for example in a movie, it can be movie attributes such as genre, year, director, actor, etc.) except users’ historical preference on a set of items. Since it’s based on historical data, the core assumption here is that the users who have agreed in the past tend to also agree in the future (Luo, 2019), and the core idea behind such systems is that the historical data of the users should be enough to make a prediction. We don’t need anything more than that historical data, no extra push from the user, no presently trending information, etc. Collaborative filtering (CF) collects information about a user by asking them to rate items and makes recommendations based on highly-rated items by users with similar tastes. CF approaches make recommendations based on the ratings of items by a set of users (neighbors) whose rating profiles are most similar to that of the target user. CF algorithms generally compute the overall similarity or correlation between users and use that as a weight when making recommendations. In a book recommendation application, for example, the first step for the CF system is to try to find the “neighbors” of the target user. The “neighbors” refer to other users who have similar tastes in books (rate the same books similarly). In the second step, only the books that are highly rated by the “neighbors” would be recommended (Zhou et al., 2011). Collaborative filtering (CF) can be categorized into two main methods as user-based collaborative filtering (memory-based) and item-based collaborative filtering (model-based) (Lee, 2015).</code></pre>
</div>
<div id="user-based-collaborative-filtering-ubcf" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> 2.2.1 User-based Collaborative Filtering (UBCF)</h2>
<pre><code>The user-based collaborative filtering approach is to predict items to the target user that are already items of interest for other users who are similar to the target user. UBCF needs the explicit rating scores of items rated by users to calculate similarities between users and exploits k-nearest neighbor algorithms to find the nearest neighbors based on user similarities. And then, it generates prediction in terms of items by combining the neighbor user’s rating scores based on similarity weighted averaging (Lee, 2015).</code></pre>
</div>
<div id="item-based-collaborative-filtering-ibcf" class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> 2.2.2 Item-based Collaborative Filtering (IBCF)</h2>
<pre><code>The item-based collaborative filtering approach is to predict items by inquiring into similarities between the items and other items that are already associated with the user. IBCF needs a set of items that the target user has already rated to calculate similarities between items and a target item. And then, it generates predictions in terms of the target item by combining the target user’s previous preferences based on these item similarities. In IBCF, users’ preference data can be collected in two ways. One is that the user explicitly gives a rating score to an item within a certain numerical scale. The other is that it implicitly analyzes users’ purchase records or click-through rates (Lee, 2015). </code></pre>
</div>
<div id="clustering" class="section level2" number="2.5">
<h2><span class="header-section-number">2.5</span> 2.3 Clustering</h2>
<pre><code>Clustering algorithms aim to group the fingerprints in classes of similar elements. The clustering requires the concept of a metric. These algorithms implement the straightforward assumption that similar data belongs to the same class. In cluster analysis, no assumption is made about the number of classes and their structure (statistical distribution); rather, the number of classes can be defined from the result of the analysis. Several different clustering techniques have been developed (di Natale &amp; Martinelli, 2019): partition-based algorithms hierarchical-based algorithms, density-based algorithms, and grid-based algorithms. </code></pre>
</div>
<div id="partition-based-algorithms" class="section level2" number="2.6">
<h2><span class="header-section-number">2.6</span> 2.3.1 Partition-based Algorithms</h2>
<pre><code>Partitional clustering assigns a set of data points into k-clusters by using iterative processes. In these processes, n data are classified into k-clusters. The predefined criterion function J assigns the datum into kth number set according to the maximization and minimization calculation in k sets.</code></pre>
</div>
<div id="k-means-algorithm" class="section level2" number="2.7">
<h2><span class="header-section-number">2.7</span> 2.3.1.1 K-means Algorithm</h2>
<pre><code>The K-Means clusters were first developed by Mac Queen (1967). In the K-Means clusters, clusters are formed using Euclidean distance. In the K-Means algorithm, unsupervised learning is used and k classes are created which minimizes the error function. In K-Means clustering, k cluster centers are created from the selected data set. It is then placed at the nearest cluster using Euclidean distance. New cluster centers are formed according to the results of the clustering. From the calculations of the clustering, the cluster center is recalculated. The arithmetic average is used as the calculation method, and the new cluster center is determined. All samples are reclassified according to the new center. This process is repeated until it is determined that the samples in the set have not passed to another set.</code></pre>
<p>The partitioning of the k pieces of data x is represented by the minimization of the J parameter as in,</p>
<pre><code>                                                                                                   .                            (equation 1) </code></pre>
<p>If the data are classified in a cluster near the center of the nearest cluster, the J value will be the minimum. If the data x are classified in the kth number cluster, the value can be optimized by changing the weighting value of wx to obtain the minimum J value. dist(x,ok) is the notation that represents the distance function. In this formula, x represents the pixel data, ok is the center of the cluster.
The distance function can be calculated as in Euclidean distance for k clusters in the next formula (Kutbay &amp; Uğurhan, 2018)
. (equation 2)</p>
</div>
<div id="hierarchical-based-algorithms" class="section level2" number="2.8">
<h2><span class="header-section-number">2.8</span> 2.3.2 Hierarchical-based Algorithms</h2>
<pre><code>In data mining and statistics, hierarchical clustering analysis is a method of cluster analysis that seeks to build a hierarchy of clusters i.e. tree-type structure based on the hierarchy. There are two types of hierarchical cluster analysis strategies, agglomerative clustering, and divisive clustering (GeeksforGeeks, 2021).</code></pre>
</div>
<div id="agglomerative-clustering" class="section level2" number="2.9">
<h2><span class="header-section-number">2.9</span> 2.3.2.1 Agglomerative Clustering</h2>
<pre><code>Agglomerative Clustering: Also known as bottom-up approach or hierarchical agglomerative clustering (HAC). A structure that is more informative than the unstructured set of clusters returned by flat clustering. This clustering algorithm does not require us to pre-specify the number of clusters. Bottom-up algorithms treat each data as a singleton cluster at the outset and then successively agglomerates pairs of clusters until all clusters have been merged into a single cluster that contains all data (GeeksforGeeks, 2021).</code></pre>
</div>
<div id="divisive-clustering" class="section level2" number="2.10">
<h2><span class="header-section-number">2.10</span> 2.3.2.2 Divisive Clustering</h2>
<pre><code>Divisive clustering: Also known as a top-down approach. This algorithm also does not require pre-specify the number of clusters. Top-down clustering requires a method for splitting a cluster that contains the whole data and proceeds by splitting clusters recursively until individual data have been split into singleton clusters (GeeksforGeeks, 2021).</code></pre>
</div>
<div id="density-based-algorithms" class="section level2" number="2.11">
<h2><span class="header-section-number">2.11</span> 2.3.3 Density-based Algorithms</h2>
<pre><code>Density-Based Clustering refers to unsupervised learning methods that identify distinctive groups/clusters in the data, based on the idea that a cluster in data space is a contiguous region of high point density, separated from other such clusters by contiguous regions of low point density (University of New South Wales, 2011). The data points in the separating regions of low point density are typically considered noise/outliers. The clusters created in these methods can be of arbitrary shape. Following are the examples of Density-based clustering algorithms (Sharma, 2021).</code></pre>
</div>
<div id="dbscan-density-based-spatial-clustering-of-applications-with-noise" class="section level2" number="2.12">
<h2><span class="header-section-number">2.12</span> 2.3.3.1 DBSCAN (Density-Based Spatial Clustering of Applications with Noise)</h2>
<pre><code>DBSCAN groups data points together based on the distance metric and criterion for a minimum number of data points. It takes two parameters – eps and minimum points. Eps indicates how close the data points should be to be considered as neighbors. The criterion for minimum points should be completed to consider that region as a dense region (Sharma, 2021).</code></pre>
</div>
<div id="optics-ordering-points-to-identify-clustering-structure" class="section level2" number="2.13">
<h2><span class="header-section-number">2.13</span> 2.3.3.2 OPTICS (Ordering Points to Identify Clustering Structure)</h2>
<pre><code>It is similar in process to DBSCAN, but it attends to one of the drawbacks of the former algorithm i.e. inability to form clusters from data of arbitrary density. It considers two more parameters which are core distance and reachability distance. Core distance indicates whether the data point being considered is core or not by setting a minimum value for it.</code></pre>
<p>Reachability distance is the maximum of core distance and the value of distance metric that is used for calculating the distance among two data points. One thing to consider about reachability distance is that its value remains not defined if one of the data points is a core point (Sharma, 2021).
2.3.3.3 HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise)
HDBSCAN is a density-based clustering method that extends the DBSCAN methodology by converting it to a hierarchical clustering algorithm (Sharma, 2021).
## 2.3.4 Grid-based Algorithms
In grid-based clustering, the data set is represented into a grid structure that comprises of grids (also called cells). The overall approach in the algorithms of this method differs from the rest of the algorithms.
They are more concerned with the value space surrounding the data points rather than the data points themselves. One of the greatest advantages of these algorithms is their reduction in computational complexity. This makes it appropriate for dealing with humongous data sets.
After partitioning the data sets into cells, it computes the density of the cells which helps in identifying the clusters. A few algorithms based on grid-based clustering are as follows (Sharma, 2021). : –
## 2.3.4.1 STING (Statistical Information Grid Approach)
In STING, the data set is divided recursively in a hierarchical manner. Each cell is further sub-divided into a different number of cells. It captures the statistical measures of the cells which helps in answering the queries in a small amount of time (Sharma, 2021).
## 2.3.4.2 Wave Cluster
In this algorithm, the data space is represented in form of wavelets. The data space composes an n-dimensional signal which helps in identifying the clusters. The parts of the signal with a lower frequency and high amplitude indicate that the data points are concentrated. These regions are identified as clusters by the algorithm. The parts of the signal where the frequency high represents the boundaries of the clusters (Sharma, 2021).
## 2.3.4.3 CLIQUE (Clustering in Quest)
CLIQUE is a combination of density-based and grid-based clustering algorithm. It partitions the data space and identifies the sub-spaces using the Apriori principle. It identifies the clusters by calculating the densities of the cells (Sharma, 2021)</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="summary-and-conclusion.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
